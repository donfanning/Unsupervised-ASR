dirs:
    train:
        data: /home/user/easton/data/TIMIT/train_phone39.csv
        tfdata: /home/user/easton/data/TIMIT/tfdata/tmp/train_39
    dev:
        data: /home/user/easton/data/TIMIT/test_phone39.csv
        tfdata: /home/user/easton/data/TIMIT/tfdata/tmp/dev_39
    test:
        data: /home/user/easton/data/TIMIT/test_phone39.csv
    type: csv
    vocab: /home/user/easton/data/TIMIT/phone39.list
    ngram: /home/user/easton/projects/EODM/all.ngram
    # ngram: /home/user/easton/projects/EODM/all.2gram
    # ngram: /home/user/easton/projects/EODM/all.3gram
    # ngram: /home/user/easton/projects/EODM/all.4gram
    # ngram: /home/user/easton/projects/EODM/all.6gram
    # ngram: /home/user/easton/projects/EODM/all.9gram
    restore: /home/user/easton/projects/EODM/models/FC_phone_level/init_checkpoint
    # restore: /home/user/easton/projects/EODM/models/FC_phone_level/checkpoint

data:
    featType: fbank
    dim_raw_input: 13
    num_context: 5
    downsample: 1
    add_delta: True
    unit: word
    full_align: True
    ngram: 5
    top_k: 1000
    k: 1000

model:
    structure: fc
    training_type: teacher-forcing
    loss_type: CE
    num_hidden: 128
    num_layers: 1

opti:
    type: adam
    warmup_steps: 100
    peak: 0.001
    decay_steps: 2000
    beam_size: 1

dev_step: 50
decode_step: 10
save_step: 50

gpus: '0'
# gpus: '1,2,3'
batch_size: 1500
num_batch_tokens: 10
bucket_boundaries: 1,2,3
num_epochs: 100000
num_steps: 500000

length_penalty_weight: 0.0
lambda_fs: 0.000001

grad_clip_global_norm: 0.0
