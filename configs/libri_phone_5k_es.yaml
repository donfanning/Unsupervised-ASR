dirs:
    train:
        data: /home/user/easton/data/LibriSpeech/train-clean-100.phone.5k.csv
        # data: /home/user/easton/data/LibriSpeech/tfdata/train_39/1.csv
        tfdata: /home/user/easton/data/LibriSpeech/tfdata/train_5k
    dev:
        data: /home/user/easton/data/LibriSpeech/dev1000.phone.csv
        tfdata: /home/user/easton/data/LibriSpeech/tfdata/dev
    test:
        data: /home/user/easton/data/LibriSpeech/dev1000.phone.csv
    type: csv
    vocab: /home/user/easton/data/LibriSpeech/phones_42.txt
    # ngram: /home/user/easton/data/LibriSpeech/100h_5k.6gram
    ngram: /home/user/easton/data/LibriSpeech/100h_5k.5gram
    # restore: /home/user/easton/projects/EODM/models/libri_phone_5k/checkpoint_supervised/

data:
    featType: mfcc
    dim_raw_input: 13
    num_context: 5
    downsample: 4
    add_delta: True
    unit: phone
    full_align: True
    ngram: 5
    top_k: 5000
    k: 5000

model:
    structure: fc
    training_type: teacher-forcing
    loss_type: CE
    num_hidden: 128
    num_layers: 1

opti:
    lr: 0.005
    population: 50
    sigma: 0.02

dev_step: 10
decode_step: 50
save_step: 20

gpus: '0'
# gpus: '0,1,2,3'
num_batch_tokens: 120000
bucket_boundaries: 81,91,101,111,122,134,149,169,224,363,1000
batch_size: 500
batch_multi: 5
num_epochs: 100000
num_steps: 500000

length_penalty_weight: 0.0
lambda_fs: 0.000001

grad_clip_global_norm: 0.0
