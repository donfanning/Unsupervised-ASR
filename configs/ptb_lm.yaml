dirs:
    train:
        data: /home/user/easton/data/PTB/train
        tfdata: /home/user/easton/data/TIMIT/tfdata/train_39
    dev:
        data: /home/user/easton/data/PTB/valid
        tfdata: /home/user/easton/data/TIMIT/tfdata/dev_39
    test:
        data: /home/user/easton/data/PTB/test
    type: csv
    vocab: /home/user/easton/data/PTB/vocab_2k.txt
    # restore: /home/user/easton/projects/EODM/models/timit_test/checkpoint

data:
    dim_embedding: 512
    unit: word

model:
    structure: lstm
    training_type: teacher-forcing
    loss_type: CE
    num_hidden: 256
    num_layers: 3

opti:
    type: adam
    warmup_steps: 100
    peak: 0.001
    decay_steps: 2000

dev_step: 50
decode_step: 200
save_step: 100

gpus: '0'
# gpus: '1,2,3'
batch_size: 1000
num_batch_tokens: 10
bucket_boundaries: 20,30,40,50,60,70,80,90
num_epochs: 100000
num_steps: 500000

grad_clip_global_norm: 0.0
